{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns \n",
    "from matplotlib.gridspec import GridSpec\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import sem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):             # JSON 데이터 로드 함수\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:  # 인코딩을 UTF-8로 지정\n",
    "        json.dump(data, f, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "raw_contents_info = pd.read_csv(\"military_data_62.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우울 척도 점수 변화\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 우울 척도 변화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 및 사후 CESD-10-D 점수를 추출하는 함수\n",
    "def extract_pre_post_scores_from_csv(data):\n",
    "    pre_scores = []\n",
    "    post_scores = []\n",
    "    \n",
    "    # 'PRE_SCORE'와 'POST_SCORE'에서 점수를 추출\n",
    "    for _, row in data.iterrows():\n",
    "        try:\n",
    "            # 사전 점수 추출\n",
    "            pre_score = row[\"PRE_SCORE\"]\n",
    "            # 사후 점수 추출\n",
    "            post_score = row[\"POST_SCORE\"]\n",
    "            # 점수를 리스트에 추가\n",
    "            pre_scores.append(pre_score)\n",
    "            post_scores.append(post_score)\n",
    "        except KeyError:\n",
    "            # 열 이름이 없거나 잘못된 경우 건너뜀\n",
    "            continue\n",
    "        except TypeError:\n",
    "            # 값이 비어 있거나 잘못된 형식일 경우 건너뜀\n",
    "            continue\n",
    "    \n",
    "    return pre_scores, post_scores\n",
    "\n",
    "\n",
    "# 중복 제거 후 고유 사용자 수 계산 함수\n",
    "def count_unique_users(data):\n",
    "    unique_user_ids = data[\"순번\"].unique()  # '순번' 기준으로 고유 사용자 ID 추출\n",
    "    return len(unique_user_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 우울 척도 점수 차이 검정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% 신뢰구간 계산 함수\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # 표준 오차\n",
    "    n = len(data)  # 샘플 크기\n",
    "    margin = std_err * stats.t.ppf((1 + confidence) / 2, df=n-1)\n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "# CESD-10-D 통계 분석 함수 (수정됨)\n",
    "def calculate_statistics(pre_scores, post_scores):\n",
    "    if len(pre_scores) != len(post_scores):\n",
    "        raise ValueError(\"Pre-test and Post-test scores must have the same number of samples.\")\n",
    "    \n",
    "    # 평균, 표준 편차, 표준오차평균 계산\n",
    "    pre_mean = np.mean(pre_scores)\n",
    "    pre_std = np.std(pre_scores, ddof=1)\n",
    "    pre_sem = stats.sem(pre_scores)\n",
    "    post_mean = np.mean(post_scores)\n",
    "    post_std = np.std(post_scores, ddof=1)\n",
    "    post_sem = stats.sem(post_scores)\n",
    "    \n",
    "    # 점수 변화 계산\n",
    "    score_changes = np.array(post_scores) - np.array(pre_scores)\n",
    "    change_mean = np.mean(score_changes)\n",
    "    change_std = np.std(score_changes, ddof=1)\n",
    "    \n",
    "    # 95% 신뢰 구간 계산\n",
    "    lower_ci, upper_ci = calculate_confidence_interval(score_changes)\n",
    "    \n",
    "    # t-검정 수행\n",
    "    t_stat, p_value = stats.ttest_rel(pre_scores, post_scores)\n",
    "    df = len(pre_scores) - 1  # 자유도\n",
    "    \n",
    "    # 결과 딕셔너리 반환\n",
    "    return {\n",
    "        \"pre_mean\": pre_mean,\n",
    "        \"pre_std\": pre_std,\n",
    "        \"pre_sem\": pre_sem,\n",
    "        \"post_mean\": post_mean,\n",
    "        \"post_std\": post_std,\n",
    "        \"post_sem\": post_sem,\n",
    "        \"change_mean\": change_mean,\n",
    "        \"change_std\": change_std,\n",
    "        \"confidence_interval\": (lower_ci, upper_ci),\n",
    "        \"t_value\": t_stat,\n",
    "        \"p_value\": p_value,\n",
    "        \"df\": df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 우울 척도 점수 증가/감소/변화없음 분포 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CESD-10-D 변화 분석 함수\n",
    "def calculate_change_statistics(pre_scores, post_scores):\n",
    "    if len(pre_scores) != len(post_scores):\n",
    "        raise ValueError(\"Pre-test and Post-test scores must have the same number of samples.\")\n",
    "    \n",
    "    # 점수 변화 계산\n",
    "    score_changes = np.array(post_scores) - np.array(pre_scores)\n",
    "    \n",
    "    # 변화 유형 분류\n",
    "    increase = score_changes[score_changes > 0]\n",
    "    decrease = score_changes[score_changes < 0]\n",
    "    no_change = score_changes[score_changes == 0]\n",
    "    \n",
    "    # 통계 계산 함수\n",
    "    def calculate_stats(group):\n",
    "        if len(group) == 0:  # 그룹이 비어있을 경우\n",
    "            return {\n",
    "                \"count\": 0,\n",
    "                \"mean\": None,\n",
    "                \"std\": None,\n",
    "                \"sem\": None\n",
    "            }\n",
    "        return {\n",
    "            \"count\": len(group),\n",
    "            \"mean\": np.mean(group),\n",
    "            \"std\": np.std(group, ddof=1),\n",
    "            \"sem\": sem(group)\n",
    "        }\n",
    "    \n",
    "    # 그룹별 통계 계산\n",
    "    increase_stats = calculate_stats(increase)\n",
    "    decrease_stats = calculate_stats(decrease)\n",
    "    no_change_stats = calculate_stats(no_change)\n",
    "    \n",
    "    \n",
    "    # 결과 딕셔너리 반환\n",
    "    return {\n",
    "        \"increase\": increase_stats,\n",
    "        \"decrease\": decrease_stats,\n",
    "        \"no_change\": no_change_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뼈대 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_criteria(data, filter_column=None, filter_value=None):\n",
    "    \"\"\"\n",
    "    특정 기준으로 데이터를 필터링한 후 우울 척도 점수 분석을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): 전체 데이터프레임\n",
    "        filter_column (str): 필터링할 열 이름 (예: '성별', '군구분')\n",
    "        filter_value (str): 필터링할 값 (예: '남자', '육군')\n",
    "    \n",
    "    Returns:\n",
    "        dict: 분석 결과를 포함한 딕셔너리\n",
    "    \"\"\"\n",
    "    # 데이터 필터링\n",
    "    if filter_column and filter_value:\n",
    "        filtered_data = data[data[filter_column] == filter_value]\n",
    "    else:\n",
    "        filtered_data = data  # 필터링 기준이 없으면 전체 데이터 사용\n",
    "    \n",
    "    # 고유 사용자 수\n",
    "    unique_user_count = count_unique_users(filtered_data)\n",
    "\n",
    "    # 사전/사후 점수 추출\n",
    "    pre_scores, post_scores = extract_pre_post_scores_from_csv(filtered_data)\n",
    "\n",
    "    # 통계 분석 결과\n",
    "    stats_result = calculate_statistics(pre_scores, post_scores)\n",
    "\n",
    "    # 변화 분석 결과\n",
    "    change_stats = calculate_change_statistics(pre_scores, post_scores)\n",
    "\n",
    "    # 출력\n",
    "    criteria_label = f\"{filter_value} ({filter_column})\" if filter_column else \"전체 사용자\"\n",
    "    print(f\"\\n=== 분석 결과: {criteria_label} ===\")\n",
    "    print(f\"총 사용자 수 (중복 제거): {unique_user_count}\")\n",
    "    print(f\"사전 검사 평균: {stats_result['pre_mean']:.2f}, 표준편차: {stats_result['pre_std']:.2f}, 표준오차평균: {stats_result['pre_sem']:.2f}\")\n",
    "    print(f\"사후 검사 평균: {stats_result['post_mean']:.2f}, 표준편차: {stats_result['post_std']:.2f}, 표준오차평균: {stats_result['post_sem']:.2f}\")\n",
    "    print(f\"변화 평균: {stats_result['change_mean']:.2f}, 표준편차: {stats_result['change_std']:.2f}\")\n",
    "    print(f\"95% 신뢰구간: [{stats_result['confidence_interval'][0]:.2f}, {stats_result['confidence_interval'][1]:.2f}]\")\n",
    "    print(f\"t-value: {stats_result['t_value']:.2f}, p-value: {stats_result['p_value']:.4f}, df: {stats_result['df']}\")\n",
    "    \n",
    "    print(\"\\n점수 변화 분포:\")\n",
    "    for category, stats in change_stats.items():\n",
    "        label = {\"increase\": \"증가\", \"decrease\": \"감소\", \"no_change\": \"변화없음\"}.get(category, category)\n",
    "        print(f\"{label} - 인원 (명): {stats['count']}, 평균: {stats['mean']}, 표준편차: {stats['std']}, 표준오차: {stats['sem']}\")\n",
    "    \n",
    "    # 결과 반환\n",
    "    return {\n",
    "        \"unique_user_count\": unique_user_count,\n",
    "        \"statistics\": stats_result,\n",
    "        \"change_distribution\": change_stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전체 사용자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 71\n",
      "사전 검사 평균: 3.22, 표준편차: 3.20, 표준오차평균: 0.29\n",
      "사후 검사 평균: 3.36, 표준편차: 3.24, 표준오차평균: 0.29\n",
      "변화 평균: 0.14, 표준편차: 2.56\n",
      "95% 신뢰구간: [-0.32, 0.60]\n",
      "t-value: -0.60, p-value: 0.5472, df: 120\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 44, 평균: 2.5454545454545454, 표준편차: 1.9223840026643453, 표준오차: 0.28981029271905495\n",
      "감소 - 인원 (명): 35, 평균: -2.7142857142857144, 표준편차: 1.6009450990224596, 표준오차: 0.2706091124051196\n",
      "변화없음 - 인원 (명): 42, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "overall_result = analyze_by_criteria(raw_contents_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 성별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 분석 결과: 남자 (성별) ===\n",
      "총 사용자 수 (중복 제거): 65\n",
      "사전 검사 평균: 3.18, 표준편차: 3.21, 표준오차평균: 0.30\n",
      "사후 검사 평균: 3.34, 표준편차: 3.21, 표준오차평균: 0.30\n",
      "변화 평균: 0.17, 표준편차: 2.59\n",
      "95% 신뢰구간: [-0.31, 0.65]\n",
      "t-value: -0.69, p-value: 0.4934, df: 113\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 42, 평균: 2.5714285714285716, 표준편차: 1.964847876478062, 표준오차: 0.3031826095064675\n",
      "감소 - 인원 (명): 33, 평균: -2.696969696969697, 표준편차: 1.6295100583620312, 표준오차: 0.2836612913158948\n",
      "변화없음 - 인원 (명): 39, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 분석 결과: 여자 (성별) ===\n",
      "총 사용자 수 (중복 제거): 6\n",
      "사전 검사 평균: 4.00, 표준편차: 3.11, 표준오차평균: 1.18\n",
      "사후 검사 평균: 3.71, 표준편차: 3.95, 표준오차평균: 1.49\n",
      "변화 평균: -0.29, 표준편차: 2.14\n",
      "95% 신뢰구간: [-2.26, 1.69]\n",
      "t-value: 0.35, p-value: 0.7358, df: 6\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 2, 평균: 2.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "감소 - 인원 (명): 2, 평균: -3.0, 표준편차: 1.4142135623730951, 표준오차: 1.0\n",
      "변화없음 - 인원 (명): 3, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "male_result = analyze_by_criteria(raw_contents_info, filter_column=\"성별\", filter_value=\"남자\")\n",
    "female_result = analyze_by_criteria(raw_contents_info, filter_column=\"성별\", filter_value=\"여자\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 군소속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 육군 분석 ===\n",
      "\n",
      "=== 분석 결과: 육군 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 61\n",
      "사전 검사 평균: 3.18, 표준편차: 3.28, 표준오차평균: 0.32\n",
      "사후 검사 평균: 3.35, 표준편차: 3.30, 표준오차평균: 0.32\n",
      "변화 평균: 0.17, 표준편차: 2.50\n",
      "95% 신뢰구간: [-0.31, 0.66]\n",
      "t-value: -0.70, p-value: 0.4842, df: 104\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 37, 평균: 2.5675675675675675, 표준편차: 2.0485840711810432, 표준오차: 0.3367851467120495\n",
      "감소 - 인원 (명): 30, 평균: -2.566666666666667, 표준편차: 1.4546793303071945, 표준오차: 0.2655868943819196\n",
      "변화없음 - 인원 (명): 38, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 공군 분석 ===\n",
      "\n",
      "=== 분석 결과: 공군 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 4\n",
      "사전 검사 평균: 4.71, 표준편차: 3.25, 표준오차평균: 1.23\n",
      "사후 검사 평균: 4.14, 표준편차: 2.91, 표준오차평균: 1.10\n",
      "변화 평균: -0.57, 표준편차: 3.74\n",
      "95% 신뢰구간: [-4.03, 2.88]\n",
      "t-value: 0.40, p-value: 0.6997, df: 6\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 3, 평균: 2.3333333333333335, 표준편차: 1.5275252316519468, 표준오차: 0.881917103688197\n",
      "감소 - 인원 (명): 2, 평균: -5.5, 표준편차: 2.1213203435596424, 표준오차: 1.4999999999999998\n",
      "변화없음 - 인원 (명): 2, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 해군 분석 ===\n",
      "\n",
      "=== 분석 결과: 해군 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 3\n",
      "사전 검사 평균: 1.33, 표준편차: 1.53, 표준오차평균: 0.88\n",
      "사후 검사 평균: 1.00, 표준편차: 1.73, 표준오차평균: 1.00\n",
      "변화 평균: -0.33, 표준편차: 0.58\n",
      "95% 신뢰구간: [-1.77, 1.10]\n",
      "t-value: 1.00, p-value: 0.4226, df: 2\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n",
      "감소 - 인원 (명): 1, 평균: -1.0, 표준편차: nan, 표준오차: nan\n",
      "변화없음 - 인원 (명): 2, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 해병대 분석 ===\n",
      "\n",
      "=== 분석 결과: 해병대 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 1\n",
      "사전 검사 평균: 3.00, 표준편차: 1.41, 표준오차평균: 1.00\n",
      "사후 검사 평균: 2.00, 표준편차: 2.83, 표준오차평균: 2.00\n",
      "변화 평균: -1.00, 표준편차: 4.24\n",
      "95% 신뢰구간: [-39.12, 37.12]\n",
      "t-value: 0.33, p-value: 0.7952, df: 1\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 1, 평균: 2.0, 표준편차: nan, 표준오차: nan\n",
      "감소 - 인원 (명): 1, 평균: -4.0, 표준편차: nan, 표준오차: nan\n",
      "변화없음 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n",
      "\n",
      "=== 국직 분석 ===\n",
      "\n",
      "=== 분석 결과: 국직 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 0\n",
      "사전 검사 평균: nan, 표준편차: nan, 표준오차평균: nan\n",
      "사후 검사 평균: nan, 표준편차: nan, 표준오차평균: nan\n",
      "변화 평균: nan, 표준편차: nan\n",
      "95% 신뢰구간: [nan, nan]\n",
      "t-value: nan, p-value: nan, df: -1\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n",
      "감소 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n",
      "변화없음 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n",
      "\n",
      "=== 국방부 분석 ===\n",
      "\n",
      "=== 분석 결과: 국방부 (군구분) ===\n",
      "총 사용자 수 (중복 제거): 2\n",
      "사전 검사 평균: 3.25, 표준편차: 2.06, 표준오차평균: 1.03\n",
      "사후 검사 평균: 4.75, 표준편차: 3.30, 표준오차평균: 1.65\n",
      "변화 평균: 1.50, 표준편차: 2.52\n",
      "95% 신뢰구간: [-2.50, 5.50]\n",
      "t-value: -1.19, p-value: 0.3189, df: 3\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 3, 평균: 2.6666666666666665, 표준편차: 1.1547005383792515, 표준오차: 0.6666666666666666\n",
      "감소 - 인원 (명): 1, 평균: -2.0, 표준편차: nan, 표준오차: nan\n",
      "변화없음 - 인원 (명): 0, 평균: None, 표준편차: None, 표준오차: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\ppjw0\\AppData\\Local\\Temp\\ipykernel_2944\\3340829375.py:27: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  \"sem\": sem(group)\n",
      "c:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "C:\\Users\\ppjw0\\AppData\\Local\\Temp\\ipykernel_2944\\4119965451.py:17: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  pre_sem = stats.sem(pre_scores)\n",
      "C:\\Users\\ppjw0\\AppData\\Local\\Temp\\ipykernel_2944\\4119965451.py:20: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  post_sem = stats.sem(post_scores)\n",
      "C:\\Users\\ppjw0\\AppData\\Local\\Temp\\ipykernel_2944\\4119965451.py:4: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  std_err = stats.sem(data)  # 표준 오차\n",
      "C:\\Users\\ppjw0\\AppData\\Local\\Temp\\ipykernel_2944\\4119965451.py:31: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  t_stat, p_value = stats.ttest_rel(pre_scores, post_scores)\n"
     ]
    }
   ],
   "source": [
    "# 군소속별 분석\n",
    "services = [\"육군\", \"공군\", \"해군\", \"해병대\", \"국직\", \"국방부\"]\n",
    "\n",
    "results_by_service = {}\n",
    "for service in services:\n",
    "    print(f\"\\n=== {service} 분석 ===\")\n",
    "    results_by_service[service] = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=service)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 장병/간부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 장병 분석 ===\n",
      "\n",
      "=== 분석 결과: 장병 (구분) ===\n",
      "총 사용자 수 (중복 제거): 49\n",
      "사전 검사 평균: 2.93, 표준편차: 2.96, 표준오차평균: 0.32\n",
      "사후 검사 평균: 3.28, 표준편차: 3.11, 표준오차평균: 0.33\n",
      "변화 평균: 0.35, 표준편차: 2.59\n",
      "95% 신뢰구간: [-0.20, 0.90]\n",
      "t-value: -1.28, p-value: 0.2056, df: 87\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 36, 평균: 2.5833333333333335, 표준편차: 2.0336455654106214, 표준오차: 0.3389409275684369\n",
      "감소 - 인원 (명): 24, 평균: -2.5833333333333335, 표준편차: 1.4719601443879746, 표준오차: 0.30046260628866583\n",
      "변화없음 - 인원 (명): 28, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 간부 분석 ===\n",
      "\n",
      "=== 분석 결과: 간부 (구분) ===\n",
      "총 사용자 수 (중복 제거): 22\n",
      "사전 검사 평균: 4.00, 표준편차: 3.71, 표준오차평균: 0.65\n",
      "사후 검사 평균: 3.58, 표준편차: 3.61, 표준오차평균: 0.63\n",
      "변화 평균: -0.42, 표준편차: 2.42\n",
      "95% 신뢰구간: [-1.28, 0.44]\n",
      "t-value: 1.01, p-value: 0.3223, df: 32\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 8, 평균: 2.375, 표준편차: 1.407885953173359, 표준오차: 0.4977628523130841\n",
      "감소 - 인원 (명): 11, 평균: -3.0, 표준편차: 1.8973665961010275, 표준오차: 0.5720775535473553\n",
      "변화없음 - 인원 (명): 14, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 장병/간부에 따른 분석\n",
    "roles = [\"장병\", \"간부\"]\n",
    "\n",
    "results_by_role = {}\n",
    "for role in roles:\n",
    "    print(f\"\\n=== {role} 분석 ===\")\n",
    "    results_by_role[role] = analyze_by_criteria(raw_contents_info, filter_column=\"구분\", filter_value=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입대(임관년도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2020년 이전 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 14\n",
      "사전 검사 평균: 4.30, 표준편차: 3.72, 표준오차평균: 0.78\n",
      "사후 검사 평균: 4.39, 표준편차: 3.94, 표준오차평균: 0.82\n",
      "변화 평균: 0.09, 표준편차: 1.81\n",
      "95% 신뢰구간: [-0.69, 0.87]\n",
      "t-value: -0.23, p-value: 0.8196, df: 22\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 6, 평균: 2.3333333333333335, 표준편차: 1.0327955589886444, 표준오차: 0.4216370213557839\n",
      "감소 - 인원 (명): 5, 평균: -2.4, 표준편차: 1.140175425099138, 표준오차: 0.5099019513592785\n",
      "변화없음 - 인원 (명): 12, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 2020~2022년 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 22\n",
      "사전 검사 평균: 2.60, 표준편차: 3.04, 표준오차평균: 0.55\n",
      "사후 검사 평균: 2.60, 표준편차: 2.54, 표준오차평균: 0.46\n",
      "변화 평균: 0.00, 표준편차: 3.06\n",
      "95% 신뢰구간: [-1.14, 1.14]\n",
      "t-value: 0.00, p-value: 1.0000, df: 29\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 12, 평균: 2.8333333333333335, 표준편차: 1.696699112626596, 표준오차: 0.4897948447043822\n",
      "감소 - 인원 (명): 11, 평균: -3.090909090909091, 표준편차: 1.9725387425622571, 표준오차: 0.5947428085016773\n",
      "변화없음 - 인원 (명): 7, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 2023년 이후 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 35\n",
      "사전 검사 평균: 3.13, 표준편차: 3.04, 표준오차평균: 0.37\n",
      "사후 검사 평균: 3.35, 표준편차: 3.21, 표준오차평균: 0.39\n",
      "변화 평균: 0.22, 표준편차: 2.57\n",
      "95% 신뢰구간: [-0.40, 0.84]\n",
      "t-value: -0.71, p-value: 0.4811, df: 67\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 26, 평균: 2.4615384615384617, 표준편차: 2.1950994370327597, 표준오차: 0.4304944178348684\n",
      "감소 - 인원 (명): 19, 평균: -2.5789473684210527, 표준편차: 1.5024346712987118, 표준오차: 0.34468215270857017\n",
      "변화없음 - 인원 (명): 23, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 분류 기준 설정\n",
    "year_categories = {\n",
    "    \"2020년 이전\": (raw_contents_info[\"임관년도\"] < 2021),\n",
    "    \"2020~2022년\": (raw_contents_info[\"임관년도\"] >= 2021) & (raw_contents_info[\"임관년도\"] <= 2022),\n",
    "    \"2023년 이후\": (raw_contents_info[\"임관년도\"] > 2022)\n",
    "}\n",
    "# 입대(임관)년도별 분석\n",
    "results_by_year = {}\n",
    "for label, condition in year_categories.items():\n",
    "    print(f\"\\n=== {label} 분석 ===\")\n",
    "    filtered_data = raw_contents_info[condition]\n",
    "    results_by_year[label] = analyze_by_criteria(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출생년도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1995년 이전 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 21\n",
      "사전 검사 평균: 4.09, 표준편차: 3.64, 표준오차평균: 0.62\n",
      "사후 검사 평균: 3.82, 표준편차: 3.77, 표준오차평균: 0.65\n",
      "변화 평균: -0.26, 표준편차: 2.30\n",
      "95% 신뢰구간: [-1.07, 0.54]\n",
      "t-value: 0.67, p-value: 0.5069, df: 33\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 9, 평균: 2.3333333333333335, 표준편차: 1.3228756555322954, 표준오차: 0.44095855184409843\n",
      "감소 - 인원 (명): 11, 평균: -2.727272727272727, 표준편차: 1.793929156399945, 표준오차: 0.5408899920234007\n",
      "변화없음 - 인원 (명): 14, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 1995~2001년 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 21\n",
      "사전 검사 평균: 2.59, 표준편차: 2.93, 표준오차평균: 0.54\n",
      "사후 검사 평균: 2.41, 표준편차: 2.43, 표준오차평균: 0.45\n",
      "변화 평균: -0.17, 표준편차: 2.51\n",
      "95% 신뢰구간: [-1.13, 0.78]\n",
      "t-value: 0.37, p-value: 0.7140, df: 28\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 10, 평균: 2.3, 표준편차: 1.1595018087284057, 표준오차: 0.36666666666666664\n",
      "감소 - 인원 (명): 9, 평균: -3.111111111111111, 표준편차: 1.7638342073763937, 표준오차: 0.5879447357921312\n",
      "변화없음 - 인원 (명): 10, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n",
      "\n",
      "=== 2002년 이후 분석 ===\n",
      "\n",
      "=== 분석 결과: 전체 사용자 ===\n",
      "총 사용자 수 (중복 제거): 29\n",
      "사전 검사 평균: 3.03, 표준편차: 3.00, 표준오차평균: 0.39\n",
      "사후 검사 평균: 3.57, 표준편차: 3.22, 표준오차평균: 0.42\n",
      "변화 평균: 0.53, 표준편차: 2.71\n",
      "95% 신뢰구간: [-0.18, 1.25]\n",
      "t-value: -1.50, p-value: 0.1385, df: 57\n",
      "\n",
      "점수 변화 분포:\n",
      "증가 - 인원 (명): 25, 평균: 2.72, 표준편차: 2.3366642891095846, 표준오차: 0.4673328578219169\n",
      "감소 - 인원 (명): 15, 평균: -2.466666666666667, 표준편차: 1.4074631010979934, 표준오차: 0.3634054100635983\n",
      "변화없음 - 인원 (명): 18, 평균: 0.0, 표준편차: 0.0, 표준오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 출생년도 분류 기준 설정\n",
    "birth_year_categories = {\n",
    "    \"1995년 이전\": (raw_contents_info[\"생년월일\"] < 1995),\n",
    "    \"1995~2001년\": (raw_contents_info[\"생년월일\"] >= 1995) & (raw_contents_info[\"생년월일\"] <= 2001),\n",
    "    \"2002년 이후\": (raw_contents_info[\"생년월일\"] > 2001)\n",
    "}\n",
    "# 출생년도별 분석\n",
    "results_by_birth_year = {}\n",
    "for label, condition in birth_year_categories.items():\n",
    "    print(f\"\\n=== {label} 분석 ===\")\n",
    "    filtered_data = raw_contents_info[condition]\n",
    "    results_by_birth_year[label] = analyze_by_criteria(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앱 사용 분포\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 콘텐츠별 이용 정보 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_content_usage(data):\n",
    "    \"\"\"\n",
    "    콘텐츠별 및 전체 콘텐츠 이용 정보를 분석합니다.\n",
    "    \"\"\"\n",
    "    content_user_stats = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # 데이터 순회\n",
    "    for _, row in data.iterrows():\n",
    "        try:\n",
    "            content_logs = json.loads(row[\"DATA\"])\n",
    "            user_id = row[\"사용자ID\"]\n",
    "            for log in content_logs:\n",
    "                content_name = log.get(\"CONTENTS_NAME\")\n",
    "                if content_name:\n",
    "                    content_user_stats[content_name][user_id] += 1\n",
    "        except (KeyError, ValueError, TypeError):\n",
    "            continue\n",
    "\n",
    "    # 콘텐츠별 통계 계산\n",
    "    content_analysis = []\n",
    "    total_users = data[\"사용자ID\"].nunique()\n",
    "    user_total_usage = defaultdict(int)  # 사용자별 모든 콘텐츠 사용 횟수 합산\n",
    "\n",
    "    for content_name, user_usage in content_user_stats.items():\n",
    "        usage_counts = list(user_usage.values())  # 사용자별 사용 횟수\n",
    "        user_count = len(user_usage)\n",
    "        total_usage = sum(usage_counts)\n",
    "        \n",
    "\n",
    "        # 사용자별 총합 업데이트\n",
    "        for user_id, count in user_usage.items():\n",
    "            user_total_usage[user_id] += count\n",
    "\n",
    "        content_analysis.append({\n",
    "            \"콘텐츠 명\": content_name,\n",
    "            \"이용 인원 (명)\": user_count,\n",
    "            \"이용 인원 비율 (%)\": round(user_count / total_users * 100, 2) if total_users > 0 else 0,\n",
    "            \"평균 (회)\": round(total_usage / user_count, 2) if user_count > 0 else 0,\n",
    "            \"표준편차\": round(pd.Series(usage_counts).std(), 2) if user_count > 1 else 0,\n",
    "            \"최솟값 (회)\": min(usage_counts) if usage_counts else 0,\n",
    "            \"최댓값 (회)\": max(usage_counts) if usage_counts else 0,\n",
    "        })\n",
    "\n",
    "    # 전체 콘텐츠 추가\n",
    "    overall_stats = {\n",
    "        \"콘텐츠 명\": \"전체 콘텐츠\",\n",
    "        \"이용 인원 (명)\": total_users,\n",
    "        \"이용 인원 비율 (%)\": 100.0,\n",
    "        \"평균 (회)\": round(sum([stat[\"평균 (회)\"] * stat[\"이용 인원 (명)\"] for stat in content_analysis]) / total_users, 2),\n",
    "        \"표준편차\": round(pd.Series(list(user_total_usage.values())).std(), 2) if len(user_total_usage) > 1 else 0,\n",
    "        \"최솟값 (회)\": min(user_total_usage.values()) if user_total_usage else 0,\n",
    "        \"최댓값 (회)\": max(user_total_usage.values()) if user_total_usage else 0,  # 사용자별 총합 최댓값 계산\n",
    "    }\n",
    "\n",
    "    content_df = pd.concat([pd.DataFrame([overall_stats]), pd.DataFrame(content_analysis)], ignore_index=True)\n",
    "    \n",
    "    # 정렬 순서를 정의\n",
    "    sort_order = [\n",
    "        \"전체 콘텐츠\",\n",
    "        \"emotion_diary\",\n",
    "        \"finding_blue_boat\",\n",
    "        \"finding_blue_fishing\",\n",
    "        \"finding_blue_parachute\",\n",
    "        \"mandala\",\n",
    "        \"mindteaching_webtoon\",\n",
    "        \"mindfulness\",\n",
    "        \"mysound_current\",\n",
    "        \"mysound_pursue\",\n",
    "        \"mysound_listen\",\n",
    "    ]\n",
    "\n",
    "    # 콘텐츠 명 컬럼을 Categorical로 설정하여 정렬\n",
    "    content_df[\"콘텐츠 명\"] = pd.Categorical(content_df[\"콘텐츠 명\"], categories=sort_order, ordered=True)\n",
    "    content_df = content_df.sort_values(\"콘텐츠 명\").reset_index(drop=True)\n",
    "\n",
    "    return content_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_criteria(data, filter_column, filter_value):\n",
    "    \"\"\"\n",
    "    특정 기준에 따라 데이터를 필터링하고 콘텐츠별 이용 정보를 분석합니다.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): 원본 데이터프레임\n",
    "        filter_column (str): 필터링할 컬럼명\n",
    "        filter_value (str): 필터링 조건 값\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 필터링된 데이터에 대한 콘텐츠별 이용 정보\n",
    "    \"\"\"\n",
    "    # 필터링된 데이터\n",
    "    filtered_data = data[data[filter_column] == filter_value]\n",
    "\n",
    "    # 분석 수행\n",
    "    return analyze_content_usage(filtered_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전체사용자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         71        100.00   37.87  48.05        2   \n",
      "1            emotion_diary         67         94.37   19.03  21.93        1   \n",
      "2        finding_blue_boat         19         26.76    2.68   3.18        1   \n",
      "3     finding_blue_fishing         25         35.21    2.56   3.00        1   \n",
      "4   finding_blue_parachute         23         32.39    3.35   3.28        1   \n",
      "5                  mandala         12         16.90    2.92   3.34        1   \n",
      "6     mindteaching_webtoon         43         60.56   19.28  24.32        1   \n",
      "7              mindfulness         44         61.97    6.61   9.02        1   \n",
      "8          mysound_current         12         16.90    2.17   2.59        1   \n",
      "9           mysound_pursue          6          8.45    3.50   4.28        1   \n",
      "10          mysound_listen          4          5.63    5.00   7.35        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6       108  \n",
      "7        42  \n",
      "8        10  \n",
      "9        12  \n",
      "10       16  \n"
     ]
    }
   ],
   "source": [
    "content_analysis_df = analyze_content_usage(raw_contents_info)\n",
    "\n",
    "# 결과 출력\n",
    "print(content_analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 성별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남성 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         65        100.00   40.41  49.43        2   \n",
      "1            emotion_diary         62         95.38   19.82  22.54        1   \n",
      "2        finding_blue_boat         18         27.69    2.72   3.27        1   \n",
      "3     finding_blue_fishing         23         35.38    2.70   3.10        1   \n",
      "4   finding_blue_parachute         21         32.31    3.57   3.36        1   \n",
      "5                  mandala         10         15.38    3.30   3.56        1   \n",
      "6     mindteaching_webtoon         43         66.15   19.28  24.32        1   \n",
      "7              mindfulness         41         63.08    7.02   9.22        1   \n",
      "8          mysound_current         11         16.92    2.27   2.69        1   \n",
      "9           mysound_pursue          5          7.69    3.60   4.77        1   \n",
      "10          mysound_listen          3          4.62    6.33   8.39        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6       108  \n",
      "7        42  \n",
      "8        10  \n",
      "9        12  \n",
      "10       16  \n",
      "\n",
      "여성 결과:\n",
      "                    콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)  표준편차  최솟값 (회)  \\\n",
      "0                  전체 콘텐츠          6        100.00   10.33  7.39        2   \n",
      "1           emotion_diary          5         83.33    9.20  7.60        1   \n",
      "2       finding_blue_boat          1         16.67    2.00  0.00        2   \n",
      "3    finding_blue_fishing          2         33.33    1.00  0.00        1   \n",
      "4  finding_blue_parachute          2         33.33    1.00  0.00        1   \n",
      "5                 mandala          2         33.33    1.00  0.00        1   \n",
      "6             mindfulness          3         50.00    1.00  0.00        1   \n",
      "7         mysound_current          1         16.67    1.00  0.00        1   \n",
      "8          mysound_pursue          1         16.67    3.00  0.00        3   \n",
      "9          mysound_listen          1         16.67    1.00  0.00        1   \n",
      "\n",
      "   최댓값 (회)  \n",
      "0       20  \n",
      "1       19  \n",
      "2        2  \n",
      "3        1  \n",
      "4        1  \n",
      "5        1  \n",
      "6        1  \n",
      "7        1  \n",
      "8        3  \n",
      "9        1  \n"
     ]
    }
   ],
   "source": [
    "# 남성에 대한 결과\n",
    "male_result = analyze_by_criteria(raw_contents_info, filter_column=\"성별\", filter_value=\"남자\")\n",
    "\n",
    "# 여성에 대한 결과\n",
    "female_result = analyze_by_criteria(raw_contents_info, filter_column=\"성별\", filter_value=\"여자\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"남성 결과:\")\n",
    "print(male_result)\n",
    "\n",
    "print(\"\\n여성 결과:\")\n",
    "print(female_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 군소속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "육군 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         61        100.00   40.47  50.89        2   \n",
      "1            emotion_diary         58         95.08   19.60  22.85        1   \n",
      "2        finding_blue_boat         17         27.87    2.35   2.96        1   \n",
      "3     finding_blue_fishing         22         36.07    2.64   3.16        1   \n",
      "4   finding_blue_parachute         20         32.79    3.00   3.11        1   \n",
      "5                  mandala         10         16.39    3.30   3.56        1   \n",
      "6     mindteaching_webtoon         41         67.21   19.80  24.80        1   \n",
      "7              mindfulness         36         59.02    7.36   9.60        1   \n",
      "8          mysound_current         11         18.03    2.18   2.71        1   \n",
      "9           mysound_pursue          6          9.84    3.50   4.28        1   \n",
      "10          mysound_listen          3          4.92    6.33   8.39        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6       108  \n",
      "7        42  \n",
      "8        10  \n",
      "9        12  \n",
      "10       16  \n",
      "\n",
      "해군 결과:\n",
      "                  콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                전체 콘텐츠          3        100.00   14.00  10.54        3   \n",
      "1         emotion_diary          3        100.00   10.33   8.08        1   \n",
      "2               mandala          1         33.33    1.00   0.00        1   \n",
      "3  mindteaching_webtoon          1         33.33    8.00   0.00        8   \n",
      "4           mindfulness          2         66.67    1.00   0.00        1   \n",
      "\n",
      "   최댓값 (회)  \n",
      "0       24  \n",
      "1       15  \n",
      "2        1  \n",
      "3        8  \n",
      "4        1  \n",
      "\n",
      "공군 결과:\n",
      "                    콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                  전체 콘텐츠          4         100.0   30.00  27.92        2   \n",
      "1           emotion_diary          4         100.0   20.75  21.65        2   \n",
      "2       finding_blue_boat          1          25.0    9.00   0.00        9   \n",
      "3    finding_blue_fishing          1          25.0    1.00   0.00        1   \n",
      "4  finding_blue_parachute          1          25.0    7.00   0.00        7   \n",
      "5             mindfulness          3          75.0    6.67   7.23        2   \n",
      "\n",
      "   최댓값 (회)  \n",
      "0       67  \n",
      "1       52  \n",
      "2        9  \n",
      "3        1  \n",
      "4        7  \n",
      "5       15  \n",
      "\n",
      "해병대 결과:\n",
      "           콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)  표준편차  최솟값 (회)  최댓값 (회)\n",
      "0         전체 콘텐츠          1         100.0    20.0     0       20       20\n",
      "1  emotion_diary          1         100.0    19.0     0       19       19\n",
      "2    mindfulness          1         100.0     1.0     0        1        1\n",
      "\n",
      "국방부 결과:\n",
      "                    콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                  전체 콘텐츠          2         100.0    19.0  16.97        7   \n",
      "1           emotion_diary          1          50.0     5.0   0.00        5   \n",
      "2       finding_blue_boat          1          50.0     2.0   0.00        2   \n",
      "3    finding_blue_fishing          2         100.0     2.5   2.12        1   \n",
      "4  finding_blue_parachute          2         100.0     5.0   5.66        1   \n",
      "5                 mandala          1          50.0     1.0   0.00        1   \n",
      "6    mindteaching_webtoon          1          50.0     9.0   0.00        9   \n",
      "7             mindfulness          2         100.0     1.5   0.71        1   \n",
      "8         mysound_current          1          50.0     2.0   0.00        2   \n",
      "9          mysound_listen          1          50.0     1.0   0.00        1   \n",
      "\n",
      "   최댓값 (회)  \n",
      "0       31  \n",
      "1        5  \n",
      "2        2  \n",
      "3        4  \n",
      "4        9  \n",
      "5        1  \n",
      "6        9  \n",
      "7        2  \n",
      "8        2  \n",
      "9        1  \n"
     ]
    }
   ],
   "source": [
    "# 군 소속별 분석\n",
    "army_result = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=\"육군\")\n",
    "navy_result = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=\"해군\")\n",
    "airforce_result = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=\"공군\")\n",
    "sea_result = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=\"해병대\")\n",
    "korea_result = analyze_by_criteria(raw_contents_info, filter_column=\"군구분\", filter_value=\"국방부\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"육군 결과:\")\n",
    "print(army_result)\n",
    "\n",
    "print(\"\\n해군 결과:\")\n",
    "print(navy_result)\n",
    "\n",
    "print(\"\\n공군 결과:\")\n",
    "print(airforce_result)\n",
    "\n",
    "print(\"\\n해병대 결과:\")\n",
    "print(sea_result)\n",
    "\n",
    "print(\"\\n국방부 결과:\")\n",
    "print(korea_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 장병/간부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장병 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         49        100.00   40.04  49.29        2   \n",
      "1            emotion_diary         48         97.96   19.62  23.81        1   \n",
      "2        finding_blue_boat         12         24.49    2.67   3.03        1   \n",
      "3     finding_blue_fishing         14         28.57    2.79   2.58        1   \n",
      "4   finding_blue_parachute         14         28.57    3.36   2.73        1   \n",
      "5                  mandala          7         14.29    1.71   1.50        1   \n",
      "6     mindteaching_webtoon         35         71.43   17.86  23.34        1   \n",
      "7              mindfulness         34         69.39    6.68   8.98        1   \n",
      "8          mysound_current          8         16.33    1.62   0.92        1   \n",
      "9           mysound_pursue          4          8.16    1.50   1.00        1   \n",
      "10          mysound_listen          3          6.12    6.33   8.39        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2         9  \n",
      "3         9  \n",
      "4         9  \n",
      "5         5  \n",
      "6       108  \n",
      "7        42  \n",
      "8         3  \n",
      "9         3  \n",
      "10       16  \n",
      "\n",
      "간부 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         22        100.00   33.04  45.90        2   \n",
      "1            emotion_diary         19         86.36   17.53  16.74        1   \n",
      "2        finding_blue_boat          7         31.82    2.71   3.68        1   \n",
      "3     finding_blue_fishing         11         50.00    2.27   3.58        1   \n",
      "4   finding_blue_parachute          9         40.91    3.33   4.18        1   \n",
      "5                  mandala          5         22.73    4.60   4.62        1   \n",
      "6     mindteaching_webtoon          8         36.36   25.50  29.13        1   \n",
      "7              mindfulness         10         45.45    6.40   9.64        1   \n",
      "8          mysound_current          4         18.18    3.25   4.50        1   \n",
      "9           mysound_pursue          2          9.09    7.50   6.36        3   \n",
      "10          mysound_listen          1          4.55    1.00   0.00        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       163  \n",
      "1        77  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6        73  \n",
      "7        30  \n",
      "8        10  \n",
      "9        12  \n",
      "10        1  \n"
     ]
    }
   ],
   "source": [
    "# 장병과 간부별 분석\n",
    "soldier_result = analyze_by_criteria(raw_contents_info, filter_column=\"구분\", filter_value=\"장병\")\n",
    "officer_result = analyze_by_criteria(raw_contents_info, filter_column=\"구분\", filter_value=\"간부\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"장병 결과:\")\n",
    "print(soldier_result)\n",
    "\n",
    "print(\"\\n간부 결과:\")\n",
    "print(officer_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입대년도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023년 이전] 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         36        100.00   29.33  35.66        2   \n",
      "1            emotion_diary         33         91.67   15.03  15.16        1   \n",
      "2        finding_blue_boat          9         25.00    2.56   3.24        1   \n",
      "3     finding_blue_fishing         15         41.67    2.73   3.53        1   \n",
      "4   finding_blue_parachute         13         36.11    3.77   3.90        1   \n",
      "5                  mandala          8         22.22    3.25   3.96        1   \n",
      "6     mindteaching_webtoon         19         52.78   15.05  18.98        1   \n",
      "7              mindfulness         20         55.56    4.60   5.32        1   \n",
      "8          mysound_current          8         22.22    2.50   3.12        1   \n",
      "9           mysound_pursue          4         11.11    4.75   4.92        1   \n",
      "10          mysound_listen          3          8.33    1.33   0.58        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       163  \n",
      "1        77  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6        73  \n",
      "7        17  \n",
      "8        10  \n",
      "9        12  \n",
      "10        2  \n",
      "\n",
      "[2023년 이후] 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         35        100.00   46.65  57.33        2   \n",
      "1            emotion_diary         34         97.14   22.91  26.61        1   \n",
      "2        finding_blue_boat         10         28.57    2.80   3.29        1   \n",
      "3     finding_blue_fishing         10         28.57    2.30   2.11        1   \n",
      "4   finding_blue_parachute         10         28.57    2.80   2.35        1   \n",
      "5                  mandala          4         11.43    2.25   1.89        1   \n",
      "6     mindteaching_webtoon         24         68.57   22.62  27.78        1   \n",
      "7              mindfulness         24         68.57    8.29  11.06        1   \n",
      "8          mysound_current          4         11.43    1.50   1.00        1   \n",
      "9           mysound_pursue          2          5.71    1.00   0.00        1   \n",
      "10          mysound_listen          1          2.86   16.00   0.00       16   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2         9  \n",
      "3         8  \n",
      "4         7  \n",
      "5         5  \n",
      "6       108  \n",
      "7        42  \n",
      "8         3  \n",
      "9         1  \n",
      "10       16  \n"
     ]
    }
   ],
   "source": [
    "# 분류 기준 설정\n",
    "year_categories = {\n",
    "    \"2023년 이전\": raw_contents_info[\"임관년도\"] < 2023,\n",
    "    \"2023년 이후\": raw_contents_info[\"임관년도\"] >= 2023\n",
    "}\n",
    "\n",
    "# 분석 결과 저장 딕셔너리\n",
    "year_results = {}\n",
    "\n",
    "# 각 분류 기준에 대해 분석\n",
    "for category, condition in year_categories.items():\n",
    "    filtered_data = raw_contents_info[condition]\n",
    "    year_results[category] = analyze_content_usage(filtered_data)\n",
    "\n",
    "# 결과 출력\n",
    "for category, result in year_results.items():\n",
    "    print(f\"\\n[{category}] 결과:\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출생년도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2000년 이전] 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         32        100.00   29.53  38.70        2   \n",
      "1            emotion_diary         29         90.62   16.00  14.49        1   \n",
      "2        finding_blue_boat          9         28.12    2.44   3.24        1   \n",
      "3     finding_blue_fishing         14         43.75    2.29   3.20        1   \n",
      "4   finding_blue_parachute         11         34.38    3.73   4.15        1   \n",
      "5                  mandala          7         21.88    3.57   4.16        1   \n",
      "6     mindteaching_webtoon         15         46.88   17.33  22.80        1   \n",
      "7              mindfulness         15         46.88    4.87   8.05        1   \n",
      "8          mysound_current          5         15.62    3.00   3.94        1   \n",
      "9           mysound_pursue          1          3.12   12.00   0.00       12   \n",
      "10          mysound_listen          1          3.12    1.00   0.00        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       163  \n",
      "1        77  \n",
      "2        11  \n",
      "3        13  \n",
      "4        14  \n",
      "5        11  \n",
      "6        73  \n",
      "7        30  \n",
      "8        10  \n",
      "9        12  \n",
      "10        1  \n",
      "\n",
      "[2000년 이후] 결과:\n",
      "                     콘텐츠 명  이용 인원 (명)  이용 인원 비율 (%)  평균 (회)   표준편차  최솟값 (회)  \\\n",
      "0                   전체 콘텐츠         39        100.00   44.72  54.08        2   \n",
      "1            emotion_diary         38         97.44   21.34  26.20        1   \n",
      "2        finding_blue_boat         10         25.64    2.90   3.28        1   \n",
      "3     finding_blue_fishing         11         28.21    2.91   2.84        1   \n",
      "4   finding_blue_parachute         12         30.77    3.00   2.37        1   \n",
      "5                  mandala          5         12.82    2.00   1.73        1   \n",
      "6     mindteaching_webtoon         28         71.79   20.32  25.45        1   \n",
      "7              mindfulness         29         74.36    7.52   9.49        1   \n",
      "8          mysound_current          7         17.95    1.57   0.98        1   \n",
      "9           mysound_pursue          5         12.82    1.80   1.10        1   \n",
      "10          mysound_listen          3          7.69    6.33   8.39        1   \n",
      "\n",
      "    최댓값 (회)  \n",
      "0       291  \n",
      "1       133  \n",
      "2         9  \n",
      "3         9  \n",
      "4         7  \n",
      "5         5  \n",
      "6       108  \n",
      "7        42  \n",
      "8         3  \n",
      "9         3  \n",
      "10       16  \n"
     ]
    }
   ],
   "source": [
    "# \"생년월일\" 기준으로 데이터 분류\n",
    "birth_date_categories = {\n",
    "    \"2000년 이전\": raw_contents_info[\"생년월일\"] < 2000,\n",
    "    \"2000년 이후\": raw_contents_info[\"생년월일\"] >= 2000\n",
    "}\n",
    "\n",
    "# 분석 결과 저장 딕셔너리\n",
    "birth_date_results = {}\n",
    "\n",
    "# 각 분류 기준에 대해 분석\n",
    "for category, condition in birth_date_categories.items():\n",
    "    filtered_data = raw_contents_info[condition]\n",
    "    birth_date_results[category] = analyze_content_usage(filtered_data)\n",
    "\n",
    "# 결과 출력\n",
    "for category, result in birth_date_results.items():\n",
    "    print(f\"\\n[{category}] 결과:\")\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "콘텐츠와 우울 척도 점수 간의 상관관계\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 회귀 분석 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 분석을 위한 콘텐츠별 feature 값 처리 함수들\n",
    "\n",
    "def parse_dates(content):\n",
    "    start_time = datetime.strptime(str(content[\"START_DATE\"]), \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time = datetime.strptime(str(content[\"END_DATE\"]), \"%Y-%m-%d %H:%M:%S\")\n",
    "    return start_time, end_time\n",
    "\n",
    "def calculate_averages(lists):\n",
    "    return [\n",
    "        sum(item for item in lst if isinstance(item, (int, float))) / len(lst) if len(lst) > 0 else 0\n",
    "        for lst in lists\n",
    "    ]\n",
    "\n",
    "def process_content(data, content_name, process_function):\n",
    "    results = []\n",
    "    total_usage_time = 0\n",
    "    for content in data[\"DATA\"]:\n",
    "        if content:\n",
    "            if content['CONTENTS_NAME'] == content_name:\n",
    "                result = process_function(content)\n",
    "                results.append(result[1:])  # Exclude usage_time from results\n",
    "                total_usage_time += result[0]  # Sum usage_time\n",
    "    return total_usage_time, list(map(list, zip(*results)))  # Transpose the list of results excluding usage_time\n",
    "\n",
    "def process_content_per_group(data, content_name, process_function):\n",
    "    results = []\n",
    "    total_usage_time = 0\n",
    "    for content_data in data:\n",
    "        if content_data[\"DATA\"]:\n",
    "            for content in content_data[\"DATA\"]:\n",
    "                if content['CONTENTS_NAME'] == content_name:\n",
    "                    result = process_function(content)\n",
    "                    results.append(result[1:])  # Exclude usage_time from results\n",
    "                    total_usage_time += result[0]  # Sum usage_time\n",
    "    return total_usage_time, list(map(list, zip(*results)))  # Transpose the list of results excluding usage_time\n",
    "\n",
    "\n",
    "def generic_process(content):\n",
    "    start_time, end_time = parse_dates(content)\n",
    "    usage_time = (end_time - start_time).total_seconds()\n",
    "    return [usage_time]  # Return usage_time as the first element\n",
    "\n",
    "def emotion_diary_process(content):\n",
    "    result = generic_process(content)\n",
    "    additional_fields = [\"VALENCE\", \"AROUSAL\", \"HAPPY\", \"JOY\", \"CALM\", \"NEUTRAL\", \"DEPRESSED\", \"SAD\", \"ANGRY\", \"SUICIDE_LEVEL\"]\n",
    "    return result + [content[field] for field in additional_fields]\n",
    "\n",
    "def findingblue_process(content, content_name, score_name):\n",
    "    result = generic_process(content)\n",
    "    score = content[score_name] if math.isnan(content[score_name]) is False else 0\n",
    "    reaction_times = []\n",
    "    correct_reactions, incorrect_reactions = 0, 0\n",
    "    for raw in content[\"RAW\"]:\n",
    "        if raw[\"Reaction_Correct\"] == \"Y\":\n",
    "            reaction_time = raw[\"TimeStamp_ms\"] if content_name == 'finding_blue_fishing' else raw[\"Reaction_Time_ms\"]\n",
    "            reaction_times.append(reaction_time)\n",
    "            correct_reactions += 1\n",
    "        else:\n",
    "            incorrect_reactions += 1\n",
    "    correct_reaction_ratio = correct_reactions / (correct_reactions + incorrect_reactions) if (correct_reactions + incorrect_reactions) > 0 else 0\n",
    "    return result + [score, sum(reaction_times) / len(reaction_times) if len(reaction_times) > 0 else 0, correct_reaction_ratio]\n",
    "\n",
    "def mandala_process(content):\n",
    "    result = generic_process(content)\n",
    "    additional_fields = [\"SELECTED_MANDALA_NUMBER\", \"SELECTED_MUSIC_NUMBER\", \"SELECTED_FEELING\", \"FIRST_COLOR_TIME_MS\", \"FINISH_COLOR_TIME_MS\", \"DEGREE_OF_COMPLETION\", \"MODIFY_COUNT\", \"DIVERSITY_COLOR\"]\n",
    "    return result + [content[field] for field in additional_fields]\n",
    "\n",
    "def mindfulness_process(content):\n",
    "    result = generic_process(content)\n",
    "    additional_fields = [\"PRE_EMOTION\", \"SESSION\", \"PROGRESS\", \"POST_EMOTION\"]\n",
    "    processed_fields = [content[field] if field != \"PROGRESS\" or math.isnan(content[\"PROGRESS\"]) is False else 0 for field in additional_fields]\n",
    "    return result + processed_fields\n",
    "\n",
    "def mysound_process(content):\n",
    "    result = generic_process(content)\n",
    "    additional_fields = [\"PRE_EMOTION\", \"SELECT_BEAT\", \"N_EACH_BEAT_PLAYED\", \"SELECTED_INSTRUMENT_LIST\", \"N_EACH_INSTRUMENT_PLAYED\", \"POST_EMOTION\"]\n",
    "    processed_fields = [content[field] if field not in [\"N_EACH_BEAT_PLAYED\", \"SELECTED_INSTRUMENT_LIST\", \"N_EACH_INSTRUMENT_PLAYED\"] else sum(content[field]) for field in additional_fields]\n",
    "    return result + processed_fields\n",
    "\n",
    "def mysound_listen_process(content):\n",
    "    result = generic_process(content)\n",
    "    list_played_song_num = len(content[\"PLAY_LIST\"])\n",
    "    return result + [list_played_song_num]\n",
    "\n",
    "def compute_means(data):\n",
    "    results = defaultdict(list)\n",
    "    content_process_map = {\n",
    "        'emotion_diary': emotion_diary_process,\n",
    "        'finding_blue_boat': lambda content: findingblue_process(content, 'finding_blue_boat', 'CONGRUENCY_SCORE'),\n",
    "        'finding_blue_fishing': lambda content: findingblue_process(content, 'finding_blue_fishing', 'SOCIAL_SCORE'),\n",
    "        'finding_blue_parachute': lambda content: findingblue_process(content, 'finding_blue_parachute', 'PARA_SCORE'),\n",
    "        'mandala': mandala_process,\n",
    "        'mindfulness': mindfulness_process,\n",
    "        'mindteaching_webtoon': generic_process,\n",
    "        'mysound_current': mysound_process,\n",
    "        'mysound_pursue': mysound_process,\n",
    "        'mysound_listen': mysound_listen_process\n",
    "    }\n",
    "\n",
    "    for key, process_function in content_process_map.items():\n",
    "        total_usage_time, processed_data = process_content(data, key, process_function)\n",
    "        if processed_data:\n",
    "            averages = calculate_averages(processed_data)\n",
    "            results[key].append([total_usage_time] + averages)\n",
    "        else:\n",
    "            results[key].append([total_usage_time])\n",
    "\n",
    "    means = {key: np.mean(vals, axis=0).tolist() for key, vals in results.items()}\n",
    "    return means\n",
    "\n",
    "def compute_means_per_group(datas):\n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    content_process_map = {\n",
    "        'emotion_diary': emotion_diary_process,\n",
    "        'finding_blue_boat': lambda content: findingblue_process(content, 'finding_blue_boat', 'CONGRUENCY_SCORE'),\n",
    "        'finding_blue_fishing': lambda content: findingblue_process(content, 'finding_blue_fishing', 'SOCIAL_SCORE'),\n",
    "        'finding_blue_parachute': lambda content: findingblue_process(content, 'finding_blue_parachute', 'PARA_SCORE'),\n",
    "        'mandala': mandala_process,\n",
    "        'mindfulness': mindfulness_process,\n",
    "        'mindteaching_webtoon': generic_process,\n",
    "        'mysound_current': mysound_process,\n",
    "        'mysound_pursue': mysound_process,\n",
    "        'mysound_listen': mysound_listen_process\n",
    "    }\n",
    "\n",
    "    for index, data in enumerate(datas):\n",
    "        for key, process_function in content_process_map.items():\n",
    "            total_usage_time, processed_data = process_content_per_group(data, key, process_function)\n",
    "            if processed_data:\n",
    "                averages = calculate_averages(processed_data)\n",
    "                results[index][key].append([total_usage_time] + averages)\n",
    "            else:\n",
    "                results[index][key].append([total_usage_time])\n",
    "\n",
    "    means = {index: {key: np.mean(vals, axis=0).tolist() for key, vals in inner_dict.items()} for index, inner_dict in results.items()}\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 전처리\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    DATA 열의 JSON 데이터를 파싱하여 필요한 정보를 추출합니다.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        try:\n",
    "            content_logs = json.loads(row[\"DATA\"])\n",
    "            for log in content_logs:\n",
    "                log[\"사용자ID\"] = row[\"사용자ID\"]  # 사용자 ID 추가\n",
    "                rows.append(log)\n",
    "        except (KeyError, ValueError, TypeError):\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 2. 피처 처리\n",
    "def parse_dates(content):\n",
    "    \"\"\"\n",
    "    시작 날짜와 종료 날짜를 파싱하여 사용 시간을 계산합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = datetime.strptime(content[\"START_DATE\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        end_time = datetime.strptime(content[\"END_DATE\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        usage_time = (end_time - start_time).total_seconds()\n",
    "    except (ValueError, TypeError):\n",
    "        usage_time = 0\n",
    "    return usage_time\n",
    "\n",
    "def process_content(content, fields):\n",
    "    \"\"\"\n",
    "    특정 콘텐츠의 사용 시간과 추가 필드를 추출합니다.\n",
    "    \"\"\"\n",
    "    usage_time = parse_dates(content)\n",
    "    processed_fields = [content.get(field, 0) for field in fields]\n",
    "    return [usage_time] + processed_fields\n",
    "\n",
    "# 3. 콘텐츠별 데이터 처리\n",
    "def extract_features(preprocessed_data, content_name, fields):\n",
    "    \"\"\"\n",
    "    콘텐츠별로 데이터를 처리하여 피처를 추출합니다.\n",
    "    \"\"\"\n",
    "    # 해당 콘텐츠 이름에 해당하는 데이터 필터링\n",
    "    content_data = preprocessed_data[preprocessed_data[\"CONTENTS_NAME\"] == content_name]\n",
    "    \n",
    "    features = []\n",
    "    for _, content in content_data.iterrows():\n",
    "        # 각 콘텐츠의 사용 시간과 추가 필드 처리\n",
    "        processed = process_content(content, fields)\n",
    "        # POST_SCORE 추가\n",
    "        score = content.get(\"POST_SCORE\", 0)  # \"POST_SCORE\"가 없으면 기본값 0\n",
    "        processed.append(score)\n",
    "        features.append(processed)\n",
    "\n",
    "    # \"POST_SCORE\"를 \"score\"로 포함한 컬럼 정의\n",
    "    return pd.DataFrame(features, columns=[\"USAGE_TIME\"] + fields + [\"score\"])\n",
    "\n",
    "# 4. 회귀 분석\n",
    "def perform_regression(features, features_name):\n",
    "    \"\"\"\n",
    "    회귀 분석을 수행합니다.\n",
    "    \"\"\"\n",
    "    regression_results = {}\n",
    "    for content_name, feature_list in features_name.items():\n",
    "        features = extract_features(preprocessed_data, content_name, feature_list)\n",
    "        \n",
    "        # 종속 변수: score로 가정\n",
    "        X = features.drop(columns=[\"score\"])\n",
    "        y = features[\"score\"]\n",
    "        \n",
    "        # 결측값 및 숫자형 변환 처리\n",
    "        X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        y = y[X.index]\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\")  # 숫자로 변환, 변환 불가한 값은 NaN 처리\n",
    "        y = pd.to_numeric(y, errors=\"coerce\")\n",
    "        \n",
    "        # NaN 제거\n",
    "        X = X.dropna()\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "        # 상수항 추가 및 회귀 수행\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        regression_results[content_name] = model\n",
    "\n",
    "    return regression_results\n",
    "\n",
    "\n",
    "# 5. 콘텐츠 정의\n",
    "features_name = {\n",
    "    \"emotion_diary\": [\"USAGE_TIME\", \"USAGE_NUM\", \"VALENCE\", \"AROUSAL\", \"HAPPY\", \"JOY\", \"CALM\", \"NEUTRAL\", \"DEPRESSED\", \"SAD\", \"ANGRY\", \"SUICIDE_LEVEL\"],\n",
    "    \"finding_blue_boat\": [\"USAGE_TIME\", \"USAGE_NUM\", \"SCORE\", \"REACTION_TIME\", \"CORRECT_REACTION_RATIO\"],\n",
    "    \"finding_blue_fishing\": [\"USAGE_TIME\", \"USAGE_NUM\", \"SCORE\", \"REACTION_TIME\", \"CORRECT_REACTION_RATIO\"], \n",
    "    \"finding_blue_parachute\": [\"USAGE_TIME\", \"USAGE_NUM\", \"SCORE\", \"REACTION_TIME\", \"CORRECT_REACTION_RATIO\"],\n",
    "    \"mandala\": [\"USAGE_TIME\", \"USAGE_NUM\", \"SELECTED_MANDALA_NUMBER\", \"SELECTED_MUSIC_NUMBER\", \"SELECTED_FEELING\", \"FIRST_COLOR_TIME_MS\", \"FINISH_COLOR_TIME_MS\", \"DEGREE_OF_COMPLETION\", \"MODIFY_COUNT\", \"DIVERSITY_COLOR\"], \n",
    "    \"mindfulness\": [\"USAGE_TIME\", \"USAGE_NUM\", \"PRE_EMOTION\", \"SESSION\", \"PROGRESS\", \"POST_EMOTION\"],\n",
    "    \"mindteaching_webtoon\": [\"USAGE_TIME\", \"USAGE_NUM\"],\n",
    "    \"mysound_current\": [\"USAGE_TIME\", \"USAGE_NUM\", \"PRE_EMOTION\", \"SELECT_BEAT\", \"N_EACH_BEAT_PLAYED\", \"SELECTED_INSTRUMENT_LIST\", \"N_EACH_INSTRUMENT_PLAYED\", \"POST_EMOTION\"], \n",
    "    \"mysound_pursue\": [\"USAGE_TIME\", \"USAGE_NUM\", \"PRE_EMOTION\", \"SELECT_BEAT\", \"N_EACH_BEAT_PLAYED\", \"SELECTED_INSTRUMENT_LIST\", \"N_EACH_INSTRUMENT_PLAYED\", \"POST_EMOTION\"], \n",
    "    \"mysound_listen\": [\"USAGE_TIME\", \"USAGE_NUM\", \"PLAY_LIST\"]\n",
    "}\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "preprocessed_data = preprocess_data(raw_contents_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "perform_regression() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6. 회귀 분석 실행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m regression_results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: perform_regression() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# 6. 회귀 분석 실행\n",
    "regression_results = perform_regression(preprocessed_data, features_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m preprocess_data(raw_contents_info)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 6. 회귀 분석 실행\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m regression_results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 7. 결과 출력\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m content_name, model \u001b[38;5;129;01min\u001b[39;00m regression_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[82], line 84\u001b[0m, in \u001b[0;36mperform_regression\u001b[1;34m(features, features_name)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# 상수항 추가 및 회귀 수행\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     X \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(X)\n\u001b[1;32m---> 84\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     85\u001b[0m     regression_results[content_name] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m regression_results\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:924\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    923\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:749\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    752\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:203\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\data.py:132\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# detect where the constant is\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     check_implicit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     exog_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(exog_max)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDataError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog contains inf or nans\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ppjw0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# 7. 결과 출력\n",
    "for content_name, model in regression_results.items():\n",
    "    print(f\"[{content_name}] R_squared: {model.rsquared:.3f}\")\n",
    "    print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
